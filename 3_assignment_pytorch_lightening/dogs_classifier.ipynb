{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# pip install\n",
        "\n",
        "!pip install black>=24.8.0\n",
        "!pip install lightning[extra]>=2.4.0\n",
        "!pip install rich>=13.8.1\n",
        "!pip install tensorboard>=2.18.0\n",
        "!pip install timm>=1.0.9\n",
        "!pip install torch>=2.4.1\n",
        "!pip install torchvision>=0.19.1"
      ],
      "metadata": {
        "id": "A8-IMRUmovM4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data module - data_modules.py\n"
      ],
      "metadata": {
        "id": "51hUjy-lpqz6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-R-8iDRsodsL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "class DogsDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir: str = 'data', batch_size: int = 32):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.dataset_url = \"https://github.com/abhiyagupta/Datasets/raw/main/CNN_Datasets/dogs_classifier_dataset.zip\"\n",
        "        self.dataset_zip_path = os.path.join(self.data_dir, 'dogs_classifier_dataset.zip')\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if not os.path.exists(os.path.join(self.data_dir, 'dogs_classifier_dataset')):\n",
        "            self.download_and_extract_dataset()\n",
        "\n",
        "    def download_and_extract_dataset(self):\n",
        "        Path(self.data_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if not os.path.exists(self.dataset_zip_path):\n",
        "            print(f\"Downloading dataset from {self.dataset_url}...\")\n",
        "            try:\n",
        "                response = requests.get(self.dataset_url)\n",
        "                response.raise_for_status()  # Raises an HTTPError for bad responses\n",
        "                with open(self.dataset_zip_path, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "                print(\"Download complete!\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading the dataset: {e}\")\n",
        "                return\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(self.dataset_zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(self.data_dir)\n",
        "            print(\"Extraction complete!\")\n",
        "        except zipfile.BadZipFile:\n",
        "            print(\"Error: The downloaded file is not a valid zip file.\")\n",
        "            # Optionally, delete the invalid zip file\n",
        "            os.remove(self.dataset_zip_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting the dataset: {e}\")\n",
        "\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None):\n",
        "        # Ensure the dataset is ready\n",
        "        self.prepare_data()\n",
        "\n",
        "        # Load the data\n",
        "        dataset_path = os.path.join(self.data_dir, 'dogs_classifier_dataset')\n",
        "        full_dataset = ImageFolder(dataset_path, transform=self.transform)\n",
        "\n",
        "        # Split the dataset into training, validation, and test sets\n",
        "        train_size = int(0.8 * len(full_dataset))\n",
        "        val_size = int(0.1 * len(full_dataset))\n",
        "        test_size = len(full_dataset) - train_size - val_size\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
        "            full_dataset, [train_size, val_size, test_size]\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model architecture -model.py\n"
      ],
      "metadata": {
        "id": "Ui6ST54opvQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wA12_9xNqnuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class DogsClassifier(pl.LightningModule):\n",
        "    def __init__(self, num_classes: int = 120, learning_rate: float = 1e-3):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_acc', acc, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "        self.log('test_loss', loss, prog_bar=True)\n",
        "        self.log('test_acc', acc, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ZP9tK8kuorGu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train.py"
      ],
      "metadata": {
        "id": "hrhcGDnKp3ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# sys.path.append('/app')\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar\n",
        "# from data_module.dogs_datamodule import DogsDataModule\n",
        "# from model.dogs_classifier import DogsClassifier\n",
        "\n",
        "def main():\n",
        "    # Create data module\n",
        "    data_module = DogsDataModule(data_dir='data', batch_size=32)\n",
        "\n",
        "    # Create model\n",
        "    model = DogsClassifier(num_classes=120, learning_rate=1e-3)\n",
        "\n",
        "    # Setup logging\n",
        "    logger = TensorBoardLogger(\"logs\", name=\"dogs_classifier\")\n",
        "\n",
        "    # Setup checkpointing\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath='checkpoints',\n",
        "        filename='dogs_classifier-{epoch:02d}-{val_loss:.2f}',\n",
        "        save_top_k=3,\n",
        "        monitor='val_loss'\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=1,\n",
        "        logger=logger,\n",
        "        callbacks=[checkpoint_callback, RichProgressBar()],\n",
        "        accelerator='auto',\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.fit(model, data_module)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "id": "94MABL0WpUXP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# eval.py"
      ],
      "metadata": {
        "id": "QQl7t0tPqk65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "# from data_modules.dogs_datamodule import DogsDataModule\n",
        "# from models.dogs_classifier import DogsClassifier\n",
        "\n",
        "def main():\n",
        "    # Load data module\n",
        "    data_module = DogsDataModule(data_dir='data', batch_size=32)\n",
        "\n",
        "    # Load the best model using the last checkpoint\n",
        "    model = DogsClassifier.load_from_checkpoint(\"checkpoints/best_model.ckpt\")\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = pl.Trainer(accelerator='auto')\n",
        "\n",
        "    # Evaluate the model\n",
        "    trainer.test(model, datamodule=data_module)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "9XbMHRxSojGZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# infer.py"
      ],
      "metadata": {
        "id": "mRAXbYOrqikf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "# from models.dogs_classifier import DogsClassifier\n",
        "\n",
        "def main():\n",
        "    # Load the best model using the last checkpoint\n",
        "    model = DogsClassifier.load_from_checkpoint(\"checkpoints/best_model.ckpt\")\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare image transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    image_path = \"path/to/your/image.jpg\"\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        prediction = torch.argmax(output, dim=1)\n",
        "\n",
        "    print(f\"Predicted class: {prediction.item()}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "7n7eJKTwqFZY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_module = DogsDataModule()\n",
        "data_module.prepare_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWNZXOrLqXtl",
        "outputId": "3f3ba39e-174c-4587-8f92-1882bc5250ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n",
            "Extraction complete!\n"
          ]
        }
      ]
    }
  ]
}