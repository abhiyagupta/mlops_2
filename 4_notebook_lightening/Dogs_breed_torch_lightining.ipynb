{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c8bfa4023d74195988852e8e9cdfc35": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3dc134d621c94d308ba2f1a19acb0c40",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[37mEpoch 0/0 \u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m22/22\u001b[0m \u001b[38;5;245m0:02:53 • 0:00:00\u001b[0m \u001b[38;5;249m0.15it/s\u001b[0m \u001b[37mv_num: 8.000 train_loss: 0.011      \u001b[0m\n                                                                               \u001b[37mtrain_acc: 1.000                    \u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/0 </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">22/22</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:02:53 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.15it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 8.000 train_loss: 0.011      </span>\n                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_acc: 1.000                    </span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "3dc134d621c94d308ba2f1a19acb0c40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c10713bb5a54f4d912ff8561986080f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0d2c62013494c1ea1d64cb7dee5edb8",
              "IPY_MODEL_736d331242ba4729b4a9006d0a59b3eb",
              "IPY_MODEL_9189b95817ef421ca7211aaa34410423"
            ],
            "layout": "IPY_MODEL_ff1361d759a146cfa7cdddfd77337993"
          }
        },
        "e0d2c62013494c1ea1d64cb7dee5edb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d61114a18de443992a1ef5129d890bc",
            "placeholder": "​",
            "style": "IPY_MODEL_efca807b866d48179625fd41e5318d92",
            "value": "Testing DataLoader 0: 100%"
          }
        },
        "736d331242ba4729b4a9006d0a59b3eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab1f9b97e3c64701a2aea1358d7c23cf",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1a9bb76f56e433d8272740a1718efe0",
            "value": 5
          }
        },
        "9189b95817ef421ca7211aaa34410423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14ed71cd0c91461a96839ff7b64de317",
            "placeholder": "​",
            "style": "IPY_MODEL_6157a07bf99d4cb384863f8815ac68a0",
            "value": " 5/5 [00:12&lt;00:00,  0.41it/s]"
          }
        },
        "ff1361d759a146cfa7cdddfd77337993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "9d61114a18de443992a1ef5129d890bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efca807b866d48179625fd41e5318d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab1f9b97e3c64701a2aea1358d7c23cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1a9bb76f56e433d8272740a1718efe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14ed71cd0c91461a96839ff7b64de317": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6157a07bf99d4cb384863f8815ac68a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning[extra]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "BppPxl5b7j2n",
        "outputId": "dc83f02e-189f-4733-fd8f-21c0e925289c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning[extra]\n",
            "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning[extra]) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning[extra]) (2024.6.1)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning[extra])\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning[extra]) (24.1)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning[extra]) (2.4.1+cu121)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning[extra])\n",
            "  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning[extra]) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning[extra]) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning[extra])\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting bitsandbytes<1.0,>=0.42.0 (from lightning[extra])\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting hydra-core<2.0,>=1.2.0 (from lightning[extra])\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting jsonargparse<5.0,>=4.27.7 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"extra\"->lightning[extra])\n",
            "  Downloading jsonargparse-4.33.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: matplotlib<4.0,>3.1 in /usr/local/lib/python3.10/dist-packages (from lightning[extra]) (3.7.1)\n",
            "Collecting omegaconf<3.0,>=2.2.3 (from lightning[extra])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: rich<14.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning[extra]) (13.8.1)\n",
            "Collecting tensorboardX<3.0,>=2.2 (from lightning[extra])\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes<1.0,>=0.42.0->lightning[extra]) (1.26.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning[extra]) (3.10.5)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core<2.0,>=1.2.0->lightning[extra])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"extra\"->lightning[extra]) (0.16)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"extra\"->lightning[extra])\n",
            "  Downloading typeshed_client-2.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning[extra]) (71.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>3.1->lightning[extra]) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>3.1->lightning[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>3.1->lightning[extra]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>3.1->lightning[extra]) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>3.1->lightning[extra]) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>3.1->lightning[extra]) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>3.1->lightning[extra]) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0,>=12.3.0->lightning[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0,>=12.3.0->lightning[extra]) (2.18.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX<3.0,>=2.2->lightning[extra]) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning[extra]) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning[extra]) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning[extra]) (3.1.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning[extra]) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning[extra]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning[extra]) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning[extra]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning[extra]) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning[extra]) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning[extra]) (4.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0,>=12.3.0->lightning[extra]) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0,>3.1->lightning[extra]) (1.16.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]<5.0,>=4.27.7; extra == \"extra\"->lightning[extra]) (6.4.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.1.0->lightning[extra]) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning[extra]) (3.10)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonargparse-4.33.1-py3-none-any.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.5/208.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeshed_client-2.7.0-py3-none-any.whl (624 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=bdf6790269e56c4adb7ba9b92a4664f67eaf79c425971c0dc68b6d66169eb047\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, typeshed-client, tensorboardX, omegaconf, lightning-utilities, jsonargparse, hydra-core, torchmetrics, bitsandbytes, pytorch-lightning, lightning\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 bitsandbytes-0.44.1 hydra-core-1.3.2 jsonargparse-4.33.1 lightning-2.4.0 lightning-utilities-0.11.7 omegaconf-2.3.0 pytorch-lightning-2.4.0 tensorboardX-2.6.2.2 torchmetrics-1.4.2 typeshed-client-2.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "a357d0e659a84e738c95fc7e853ea43d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional, Union\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "EfPEkSjy7eyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DogsDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, dl_path: str = \"data\", batch_size: int = 32,num_workers: int = 0):\n",
        "        super().__init__()\n",
        "        self._dl_path = Path(dl_path)\n",
        "        self._batch_size = batch_size\n",
        "        self._num_workers = num_workers\n",
        "        self.train_dataset = None\n",
        "        self.val_dataset = None\n",
        "        self.test_dataset = None\n",
        "        self.num_classes = None\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Download images and prepare datasets.\"\"\"\n",
        "        dataset_dir = self._dl_path.joinpath(\"dataset\")\n",
        "        print(f\"Checking for dataset in: {dataset_dir}\")\n",
        "\n",
        "        # Check if the dataset already exists\n",
        "        if not dataset_dir.exists():\n",
        "            download_and_extract_archive(\n",
        "                url=\"https://raw.githubusercontent.com/abhiyagupta/Datasets/refs/heads/main/CNN_Datasets/dogs_classifier_dataset.zip\",\n",
        "                download_root=self._dl_path,\n",
        "                remove_finished=True\n",
        "            )\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None):\n",
        "        \"\"\"Load data and split into train, val, test sets.\"\"\"\n",
        "        print(\"Setting up data...\")\n",
        "        dataset_df = self.create_dataframe()\n",
        "        print(f\"Total images found: {len(dataset_df)}\")\n",
        "\n",
        "        self.num_classes = dataset_df['label'].nunique()\n",
        "        print(f\"Number of unique classes: {self.num_classes}\")\n",
        "\n",
        "        train_df, temp_df = self.split_train_temp(dataset_df)\n",
        "        val_df, test_df = self.split_val_test(temp_df)\n",
        "\n",
        "        print(f\"Train set size: {len(train_df)}\")\n",
        "        print(f\"Validation set size: {len(val_df)}\")\n",
        "        print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "\n",
        "        self.train_dataset = self.create_dataset(train_df)\n",
        "        self.val_dataset = self.create_dataset(val_df)\n",
        "        self.test_dataset = self.create_dataset(test_df)\n",
        "\n",
        "        print(f\"Length of validation dataset: {len(self.val_dataset)}\")\n",
        "\n",
        "    def create_dataframe(self):\n",
        "        DATASET_PATH = self._dl_path.joinpath(\"dataset\")\n",
        "        print(f\"Looking for images in: {DATASET_PATH}\")\n",
        "        IMAGE_PATH_LIST = list(DATASET_PATH.glob(\"*/*.jpg\"))\n",
        "        print(f\"Number of images found: {len(IMAGE_PATH_LIST)}\")\n",
        "        images_path = [str(img_path.relative_to(DATASET_PATH)) for img_path in IMAGE_PATH_LIST]\n",
        "        labels = [img_path.parent.name for img_path in IMAGE_PATH_LIST]\n",
        "\n",
        "        df = pd.DataFrame({'image_path': images_path, 'label': labels})\n",
        "        print(f\"Number of unique labels: {df['label'].nunique()}\")\n",
        "        print(df.head())\n",
        "        return df\n",
        "\n",
        "    def get_num_classes(self):\n",
        "        if self.num_classes is None:\n",
        "            raise ValueError(\"Number of classes is not set. Make sure setup() method is called.\")\n",
        "        return self.num_classes\n",
        "\n",
        "    def split_train_temp(self, df):\n",
        "        train_split_idx, temp_split_idx, _, _ = (\n",
        "            train_test_split(\n",
        "                df.index,\n",
        "                df.label,\n",
        "                test_size=0.30,\n",
        "                stratify=df.label,\n",
        "                random_state=42\n",
        "            )\n",
        "        )\n",
        "        return df.iloc[train_split_idx].reset_index(drop=True), df.iloc[temp_split_idx].reset_index(drop=True)\n",
        "\n",
        "    def split_val_test(self, df):\n",
        "        val_split_idx, test_split_idx, _, _ = (\n",
        "            train_test_split(\n",
        "                df.index,\n",
        "                df.label,\n",
        "                test_size=0.5,\n",
        "                stratify=df.label,\n",
        "                random_state=42\n",
        "            )\n",
        "        )\n",
        "        return df.iloc[val_split_idx].reset_index(drop=True), df.iloc[test_split_idx].reset_index(drop=True)\n",
        "\n",
        "    def create_dataset(self, df):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        return CustomImageDataset(\n",
        "            root=str(self._dl_path.joinpath(\"dataset\")),\n",
        "            image_paths=df['image_path'],\n",
        "            transform=transform\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self._batch_size, num_workers=self._num_workers, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self._batch_size, num_workers=self._num_workers, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self._batch_size, num_workers=self._num_workers, shuffle=False)\n",
        "\n",
        "\n",
        "class CustomImageDataset(Dataset):  # Change this line\n",
        "    def __init__(self, root, image_paths, transform=None):\n",
        "        self.root = root\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        for idx, path in enumerate(image_paths):\n",
        "            img = Image.open(os.path.join(root, path))\n",
        "            self.images.append(img)\n",
        "            self.labels.append(idx)  # Assuming labels are indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.images[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "NmU2OmNZ7eoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call class created above\n",
        "dm = DogsDataModule()\n",
        "dm.prepare_data()\n",
        "dm.setup()\n",
        "train_loader = dm.train_dataloader()\n",
        "val_loader = dm.val_dataloader()\n",
        "test_loader = dm.test_dataloader()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzBNHSfJ7eag",
        "outputId": "ee6a0942-5c81-4637-e496-2da2c70102aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for dataset in: data/dataset\n",
            "Setting up data...\n",
            "Looking for images in: data/dataset\n",
            "Number of images found: 967\n",
            "Number of unique labels: 10\n",
            "               image_path    label\n",
            "0  Bulldog/Bulldog_68.jpg  Bulldog\n",
            "1  Bulldog/Bulldog_29.jpg  Bulldog\n",
            "2  Bulldog/Bulldog_87.jpg  Bulldog\n",
            "3  Bulldog/Bulldog_51.jpg  Bulldog\n",
            "4   Bulldog/Bulldog_3.jpg  Bulldog\n",
            "Total images found: 967\n",
            "Number of unique classes: 10\n",
            "Train set size: 676\n",
            "Validation set size: 145\n",
            "Test set size: 146\n",
            "Length of validation dataset: 145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyKy8lFnOsiw",
        "outputId": "18fc403b-c5ff-4f91-ac68-4b7c9a14c897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train=dm.train_dataloader()\n",
        "print(df_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZPD80rh8xYm",
        "outputId": "d3c20b28-ded0-4824-cdb5-06efefb0ec51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x79bec4324760>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm  # Make sure timm is installed: pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bfNLcek3KROA",
        "outputId": "d0117f74-57f9-4f84-a571-3b53883894de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "Successfully installed timm-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "import timm  # Make sure timm is installed: pip install timm\n",
        "\n",
        "class DogsClassifier(pl.LightningModule):\n",
        "    def __init__(self, lr: float = 1e-3, num_classes: int = 10):\n",
        "        super().__init__()\n",
        "        self.lr = lr\n",
        "        self.num_classes = num_classes\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
        "\n",
        "        # Load pre-trained ResNet18 model, adjusting the final layer for 10 classes\n",
        "        self.model = timm.create_model('resnet18', pretrained=True, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "\n",
        "        # Check for and correct out-of-bounds labels during training\n",
        "        labels = torch.clamp(labels, 0, self.num_classes - 1)\n",
        "\n",
        "        outputs = self(inputs)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "        acc = self.accuracy(outputs, labels)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "\n",
        "        # Check for and correct out-of-bounds labels during validation\n",
        "        labels = torch.clamp(labels, 0, self.num_classes - 1)\n",
        "\n",
        "        outputs = self(inputs)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "        acc = self.accuracy(outputs, labels)\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        # Check for and correct out-of-bounds labels during validation\n",
        "        labels = torch.clamp(labels, 0, self.num_classes - 1)\n",
        "\n",
        "        outputs = self(inputs)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        acc = self.accuracy(outputs, labels)\n",
        "        self.log(\"test_loss\", loss, prog_bar=True)\n",
        "        self.log(\"test_acc\", acc, prog_bar=True)\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "BePBLzW1JqFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view model architecture\n",
        "model = DogsClassifier()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R4uu0LMb9uAM",
        "outputId": "35f86618-755c-4683-931e-bf49041d2029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DogsClassifier(\n",
            "  (criterion): CrossEntropyLoss()\n",
            "  (accuracy): MulticlassAccuracy()\n",
            "  (model): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (act1): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop_block): Identity()\n",
            "        (act1): ReLU(inplace=True)\n",
            "        (aa): Identity()\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop_block): Identity()\n",
            "        (act1): ReLU(inplace=True)\n",
            "        (aa): Identity()\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop_block): Identity()\n",
            "        (act1): ReLU(inplace=True)\n",
            "        (aa): Identity()\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop_block): Identity()\n",
            "        (act1): ReLU(inplace=True)\n",
            "        (aa): Identity()\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop_block): Identity()\n",
            "        (act1): ReLU(inplace=True)\n",
            "        (aa): Identity()\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop_block): Identity()\n",
            "        (act1): ReLU(inplace=True)\n",
            "        (aa): Identity()\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop_block): Identity()\n",
            "        (act1): ReLU(inplace=True)\n",
            "        (aa): Identity()\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (drop_block): Identity()\n",
            "        (act1): ReLU(inplace=True)\n",
            "        (aa): Identity()\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
            "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# sys.path.append('/app')\n",
        "\n",
        "import os\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar\n",
        "# from data_module.dogs_datamodule import DogsDataModule\n",
        "# from model.dogs_classifier import DogsClassifier\n",
        "import time\n",
        "\n",
        "def main():\n",
        "    # Create data module\n",
        "    data_module = DogsDataModule(dl_path='./data', batch_size=32)\n",
        "\n",
        "    # Explicitly call prepare_data and setup\n",
        "    data_module.prepare_data()\n",
        "    data_module.setup()\n",
        "\n",
        "    # Get the number of classes\n",
        "    num_classes = data_module.get_num_classes()\n",
        "    print(f\"Number of classes in the dataset: {num_classes}\")\n",
        "\n",
        "    print(\"#########################\")\n",
        "\n",
        "    # Create model\n",
        "    model = DogsClassifier(num_classes=num_classes, lr=1e-3)\n",
        "\n",
        "    # Setup logging\n",
        "    logger = TensorBoardLogger(\"logs\", name=\"dogs_classifier\")\n",
        "\n",
        "    # Ensure directory exists\n",
        "    checkpoint_dir = '/app/checkpoints/'\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "       os.makedirs(checkpoint_dir)\n",
        "\n",
        "\n",
        "\n",
        "    # Setup checkpointing\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor='val_loss',         # Save model with the best validation loss\n",
        "        dirpath= checkpoint_dir,     # Directory where checkpoints will be saved\n",
        "        filename='dogs_classifier-{epoch:02d}-{val_loss:.2f}',  # Checkpoint filename format\n",
        "        save_top_k=1,               # Save only the best model\n",
        "        mode='min',                 # 'min' because we want the minimum validation loss\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=1,\n",
        "        logger=logger,\n",
        "        callbacks=[checkpoint_callback, RichProgressBar()],\n",
        "        accelerator='auto',\n",
        "    )\n",
        "\n",
        "    print(\"trainer created\")\n",
        "\n",
        "    # Train the model\n",
        "    trainer.fit(model, data_module)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4c8bfa4023d74195988852e8e9cdfc35",
            "3dc134d621c94d308ba2f1a19acb0c40"
          ]
        },
        "collapsed": true,
        "id": "3TnvqgA0_9Wi",
        "outputId": "67c43afc-fcfa-44a6-fd29-3eca299ad343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for dataset in: data/dataset\n",
            "Setting up data...\n",
            "Looking for images in: data/dataset\n",
            "Number of images found: 967\n",
            "Number of unique labels: 10\n",
            "               image_path    label\n",
            "0  Bulldog/Bulldog_68.jpg  Bulldog\n",
            "1  Bulldog/Bulldog_29.jpg  Bulldog\n",
            "2  Bulldog/Bulldog_87.jpg  Bulldog\n",
            "3  Bulldog/Bulldog_51.jpg  Bulldog\n",
            "4   Bulldog/Bulldog_3.jpg  Bulldog\n",
            "Total images found: 967\n",
            "Number of unique classes: 10\n",
            "Train set size: 676\n",
            "Validation set size: 145\n",
            "Test set size: 146\n",
            "Number of classes in the dataset: 10\n",
            "#########################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainer created\n",
            "Checking for dataset in: data/dataset\n",
            "Setting up data...\n",
            "Looking for images in: data/dataset\n",
            "Number of images found: 967\n",
            "Number of unique labels: 10\n",
            "               image_path    label\n",
            "0  Bulldog/Bulldog_68.jpg  Bulldog\n",
            "1  Bulldog/Bulldog_29.jpg  Bulldog\n",
            "2  Bulldog/Bulldog_87.jpg  Bulldog\n",
            "3  Bulldog/Bulldog_51.jpg  Bulldog\n",
            "4   Bulldog/Bulldog_3.jpg  Bulldog\n",
            "Total images found: 967\n",
            "Number of unique classes: 10\n",
            "Train set size: 676\n",
            "Validation set size: 145\n",
            "Test set size: 146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /app/checkpoints exists and is not empty.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType              \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ criterion │ CrossEntropyLoss   │      0 │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ accuracy  │ MulticlassAccuracy │      0 │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ model     │ ResNet             │ 11.2 M │ train │\n",
              "└───┴───────────┴────────────────────┴────────┴───────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type               </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
              "┡━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ criterion │ CrossEntropyLoss   │      0 │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ accuracy  │ MulticlassAccuracy │      0 │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ model     │ ResNet             │ 11.2 M │ train │\n",
              "└───┴───────────┴────────────────────┴────────┴───────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 11.2 M                                                                                           \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 11.2 M                                                                                               \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 44                                                                         \n",
              "\u001b[1mModules in train mode\u001b[0m: 96                                                                                          \n",
              "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 11.2 M                                                                                           \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 11.2 M                                                                                               \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 44                                                                         \n",
              "<span style=\"font-weight: bold\">Modules in train mode</span>: 96                                                                                          \n",
              "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c8bfa4023d74195988852e8e9cdfc35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches \n",
              "(22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if\n",
              "you want to see logs for the training epoch.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches \n",
              "(22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if\n",
              "you want to see logs for the training epoch.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1roOqHurGRSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('/app/checkpoints/dogs_classifier-epoch=00-val_loss=0.62.ckpt')\n",
        "print(checkpoint['state_dict'].keys())  # Check the keys and shapes in the state_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeQW6P-zF0hr",
        "outputId": "4f08ea5f-729a-4275-c77c-34a0e0dd084a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['model.conv1.weight', 'model.bn1.weight', 'model.bn1.bias', 'model.bn1.running_mean', 'model.bn1.running_var', 'model.bn1.num_batches_tracked', 'model.layer1.0.conv1.weight', 'model.layer1.0.bn1.weight', 'model.layer1.0.bn1.bias', 'model.layer1.0.bn1.running_mean', 'model.layer1.0.bn1.running_var', 'model.layer1.0.bn1.num_batches_tracked', 'model.layer1.0.conv2.weight', 'model.layer1.0.bn2.weight', 'model.layer1.0.bn2.bias', 'model.layer1.0.bn2.running_mean', 'model.layer1.0.bn2.running_var', 'model.layer1.0.bn2.num_batches_tracked', 'model.layer1.1.conv1.weight', 'model.layer1.1.bn1.weight', 'model.layer1.1.bn1.bias', 'model.layer1.1.bn1.running_mean', 'model.layer1.1.bn1.running_var', 'model.layer1.1.bn1.num_batches_tracked', 'model.layer1.1.conv2.weight', 'model.layer1.1.bn2.weight', 'model.layer1.1.bn2.bias', 'model.layer1.1.bn2.running_mean', 'model.layer1.1.bn2.running_var', 'model.layer1.1.bn2.num_batches_tracked', 'model.layer2.0.conv1.weight', 'model.layer2.0.bn1.weight', 'model.layer2.0.bn1.bias', 'model.layer2.0.bn1.running_mean', 'model.layer2.0.bn1.running_var', 'model.layer2.0.bn1.num_batches_tracked', 'model.layer2.0.conv2.weight', 'model.layer2.0.bn2.weight', 'model.layer2.0.bn2.bias', 'model.layer2.0.bn2.running_mean', 'model.layer2.0.bn2.running_var', 'model.layer2.0.bn2.num_batches_tracked', 'model.layer2.0.downsample.0.weight', 'model.layer2.0.downsample.1.weight', 'model.layer2.0.downsample.1.bias', 'model.layer2.0.downsample.1.running_mean', 'model.layer2.0.downsample.1.running_var', 'model.layer2.0.downsample.1.num_batches_tracked', 'model.layer2.1.conv1.weight', 'model.layer2.1.bn1.weight', 'model.layer2.1.bn1.bias', 'model.layer2.1.bn1.running_mean', 'model.layer2.1.bn1.running_var', 'model.layer2.1.bn1.num_batches_tracked', 'model.layer2.1.conv2.weight', 'model.layer2.1.bn2.weight', 'model.layer2.1.bn2.bias', 'model.layer2.1.bn2.running_mean', 'model.layer2.1.bn2.running_var', 'model.layer2.1.bn2.num_batches_tracked', 'model.layer3.0.conv1.weight', 'model.layer3.0.bn1.weight', 'model.layer3.0.bn1.bias', 'model.layer3.0.bn1.running_mean', 'model.layer3.0.bn1.running_var', 'model.layer3.0.bn1.num_batches_tracked', 'model.layer3.0.conv2.weight', 'model.layer3.0.bn2.weight', 'model.layer3.0.bn2.bias', 'model.layer3.0.bn2.running_mean', 'model.layer3.0.bn2.running_var', 'model.layer3.0.bn2.num_batches_tracked', 'model.layer3.0.downsample.0.weight', 'model.layer3.0.downsample.1.weight', 'model.layer3.0.downsample.1.bias', 'model.layer3.0.downsample.1.running_mean', 'model.layer3.0.downsample.1.running_var', 'model.layer3.0.downsample.1.num_batches_tracked', 'model.layer3.1.conv1.weight', 'model.layer3.1.bn1.weight', 'model.layer3.1.bn1.bias', 'model.layer3.1.bn1.running_mean', 'model.layer3.1.bn1.running_var', 'model.layer3.1.bn1.num_batches_tracked', 'model.layer3.1.conv2.weight', 'model.layer3.1.bn2.weight', 'model.layer3.1.bn2.bias', 'model.layer3.1.bn2.running_mean', 'model.layer3.1.bn2.running_var', 'model.layer3.1.bn2.num_batches_tracked', 'model.layer4.0.conv1.weight', 'model.layer4.0.bn1.weight', 'model.layer4.0.bn1.bias', 'model.layer4.0.bn1.running_mean', 'model.layer4.0.bn1.running_var', 'model.layer4.0.bn1.num_batches_tracked', 'model.layer4.0.conv2.weight', 'model.layer4.0.bn2.weight', 'model.layer4.0.bn2.bias', 'model.layer4.0.bn2.running_mean', 'model.layer4.0.bn2.running_var', 'model.layer4.0.bn2.num_batches_tracked', 'model.layer4.0.downsample.0.weight', 'model.layer4.0.downsample.1.weight', 'model.layer4.0.downsample.1.bias', 'model.layer4.0.downsample.1.running_mean', 'model.layer4.0.downsample.1.running_var', 'model.layer4.0.downsample.1.num_batches_tracked', 'model.layer4.1.conv1.weight', 'model.layer4.1.bn1.weight', 'model.layer4.1.bn1.bias', 'model.layer4.1.bn1.running_mean', 'model.layer4.1.bn1.running_var', 'model.layer4.1.bn1.num_batches_tracked', 'model.layer4.1.conv2.weight', 'model.layer4.1.bn2.weight', 'model.layer4.1.bn2.bias', 'model.layer4.1.bn2.running_mean', 'model.layer4.1.bn2.running_var', 'model.layer4.1.bn2.num_batches_tracked', 'model.fc.weight', 'model.fc.bias'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-73-c5c92edbcda7>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('/app/checkpoints/dogs_classifier-epoch=00-val_loss=0.62.ckpt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from model.dogs_classifier import DogsClassifier\n",
        "# from data_module.dogs_datamodule import DogsDataModule\n",
        "# import pytorch_lightning as pl\n",
        "\n",
        "def evaluate():\n",
        "    data_module = DogsDataModule('./data')  # Ensure data path is correct\n",
        "    model = DogsClassifier.load_from_checkpoint('/app/checkpoints/dogs_classifier-epoch=00-val_loss=0.62.ckpt', strict=False)\n",
        "\n",
        "    # Use accelerator='gpu' or accelerator='auto' instead of gpus\n",
        "    trainer = pl.Trainer(accelerator='auto')\n",
        "    trainer.test(model, datamodule=data_module)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481,
          "referenced_widgets": [
            "8c10713bb5a54f4d912ff8561986080f",
            "e0d2c62013494c1ea1d64cb7dee5edb8",
            "736d331242ba4729b4a9006d0a59b3eb",
            "9189b95817ef421ca7211aaa34410423",
            "ff1361d759a146cfa7cdddfd77337993",
            "9d61114a18de443992a1ef5129d890bc",
            "efca807b866d48179625fd41e5318d92",
            "ab1f9b97e3c64701a2aea1358d7c23cf",
            "c1a9bb76f56e433d8272740a1718efe0",
            "14ed71cd0c91461a96839ff7b64de317",
            "6157a07bf99d4cb384863f8815ac68a0"
          ]
        },
        "id": "zBcuMjWNG7b6",
        "outputId": "8b2e2e7c-121d-42c2-c955-f69e88cefd67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for dataset in: data/dataset\n",
            "Setting up data...\n",
            "Looking for images in: data/dataset\n",
            "Number of images found: 967\n",
            "Number of unique labels: 10\n",
            "               image_path    label\n",
            "0  Bulldog/Bulldog_68.jpg  Bulldog\n",
            "1  Bulldog/Bulldog_29.jpg  Bulldog\n",
            "2  Bulldog/Bulldog_87.jpg  Bulldog\n",
            "3  Bulldog/Bulldog_51.jpg  Bulldog\n",
            "4   Bulldog/Bulldog_3.jpg  Bulldog\n",
            "Total images found: 967\n",
            "Number of unique classes: 10\n",
            "Train set size: 676\n",
            "Validation set size: 145\n",
            "Test set size: 146\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c10713bb5a54f4d912ff8561986080f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9383561611175537    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6568513512611389    \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9383561611175537     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6568513512611389     </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "def inference(model, image_path):\n",
        "    # Load and preprocess the image\n",
        "    img = PILImage.open(image_path).convert('RGB')\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Apply the transform to the image\n",
        "    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Move the input tensor to the same device as the model\n",
        "    img_tensor = img_tensor.to(model.device)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model = DogsClassifier.load_from_checkpoint('/app/checkpoints/dogs_classifier-epoch=00-val_loss=0.62.ckpt', strict=False)\n",
        "    model.eval()\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor)  # Use img_tensor instead of image_tensor\n",
        "        probabilities = F.softmax(output, dim=1)  # Get probabilities across the classes\n",
        "        predicted_class = torch.argmax(probabilities, dim=1).item()  # Use dim=1 for multiclass\n",
        "\n",
        "    # Get class labels (you need to provide these based on your specific dog breeds)\n",
        "    class_labels = [\"Breed1\", \"Breed2\", \"Breed3\", \"Breed4\", \"Breed5\", \"Breed6\", \"Breed7\", \"Breed8\", \"Breed9\", \"Breed10\"]  # Replace with your actual breed names\n",
        "    predicted_label = class_labels[predicted_class]\n",
        "    confidence = probabilities[0][predicted_class].item()\n",
        "\n",
        "    # Visualize the prediction\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Predicted: {predicted_label.capitalize()} (Confidence: {confidence:.2f})')\n",
        "    plt.show()\n",
        "\n",
        "    return predicted_label, confidence\n"
      ],
      "metadata": {
        "id": "hA-L-MUXkWPN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}